meta:
  title: "×‘×™× ×” ××œ××›×•×ª×™×ª: ×©×™×¢×•×¨ 02"
  author: "Tzvi Merling"
  aspectRatio: "responsive"

slides:
  # ===========================================
  # LESSON 02: Recent AI History & NLP
  # ===========================================
  - id: slide-001
    layout: title
    direction: rtl
    background: "/assets/images/lesson02_history_nlp_bg.png"
    content:
      - type: title
        text: ""


  - id: slide-002
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "×¦×™×¨ ×”×–××Ÿ ×©×œ ××•×“×œ×™ ×”×©×¤×”"
      - type: diagram
        src: "/assets/diagrams/history-timeline.svg"

  # -------------------------------------------
  # AlexNet & Convolutions (2 slides)
  # -------------------------------------------
  - id: slide-030
    layout: title
    direction: rtl
    content:
      - type: title
        text: "×¨×©×ª×•×ª ×§×•× ×‘×•×œ×•×¦×™×”"
      - type: subtitle
        text: "Convolutional Neural Networks (CNNs)"

  - id: slide-031
    layout: itemsCompact
    direction: rtl
    content:
      - type: title
        text: "AlexNet (2012)"
      - type: subtitle
        text: "×¨×’×¢ ×¤×¨×™×¦×ª ×”×“×¨×š ×©×œ ×”×œ××™×“×” ×”×¢××•×§×”"
      - type: items
        items:
          - title: "×ª×—×¨×•×ª ImageNet"
            description: "× ×™×¦×—×•×Ÿ ××•×—×¥ ×‘×–×™×”×•×™ ×ª××•× ×•×ª - ×©×’×™××” ×©×œ 15.3% ×œ×¢×•××ª 26.2% ×©×œ ×”××ª×—×¨×” ×”×§×¨×•×‘"
            icon: "ğŸ†"
          - title: "××¨×›×™×˜×§×˜×•×¨×” ×—×“×©× ×™×ª"
            description: "8 ×©×›×‘×•×ª ×¢××•×§×•×ª, 60 ××™×œ×™×•×Ÿ ×¤×¨××˜×¨×™×, ×©×™××•×© ×‘-ReLU ×•-Dropout"
            icon: "ğŸ—ï¸"
          - title: "GPU ×œ××™××•×Ÿ"
            description: "×œ×¨××©×•× ×” - ××™××•×Ÿ ×¢×œ ×©× ×™ ×›×¨×˜×™×¡×™ GTX 580, ×©×‘×•×¢×•×ª ×‘××§×•× ×—×•×“×©×™×"
            icon: "âš¡"
          - title: "×”×©×¤×¢×” ×”×™×¡×˜×•×¨×™×ª"
            description: "×”×•×›×™×— ×©×¨×©×ª×•×ª ×¢××•×§×•×ª ×¢×•×‘×“×•×ª - ×¤×ª×— ××ª ×¢×™×“×Ÿ ×”×œ××™×“×” ×”×¢××•×§×”"
            icon: "ğŸ“ˆ"

  - id: slide-032
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××¨×›×™×˜×§×˜×•×¨×ª AlexNet"
      - type: diagram
        src: "/assets/diagrams/alexnet_architecture_he.svg"
    notes: |
      AlexNet ×”×›×™×œ 5 ×©×›×‘×•×ª ×§×•× ×‘×•×œ×•×¦×™×” ×•-3 ×©×›×‘×•×ª Fully Connected.
      ×—×™×“×•×©×™×: ReLU (××”×™×¨ ×™×•×ª×¨ ×-sigmoid), Dropout (×× ×™×¢×ª overfitting), GPU training.

  - id: slide-032a
    layout: text-image
    direction: rtl
    content:
      - type: title
        text: "AlexNet - ×”××‘× ×” ×”××œ×"
      - type: bullets
        items:
          - "5 ×©×›×‘×•×ª ×§×•× ×‘×•×œ×•×¦×™×”"
          - "3 ×©×›×‘×•×ª Fully Connected"
          - "60 ××™×œ×™×•×Ÿ ×¤×¨××˜×¨×™×"
      - type: image
        src: "/assets/images/An-illustration-of-deep-CNN-based-AlexNet-model-Krizhevsky-et-al-2012.png"
        alt: "AlexNet Architecture Diagram"

  - id: slide-032b
    layout: text-image
    direction: rtl
    content:
      - type: title
        text: "ImageNet - ××©×™××ª ×”×¡×™×•×•×’"
      - type: bullets
        items:
          - "1.2 ××™×œ×™×•×Ÿ ×ª××•× ×•×ª ××™××•×Ÿ"
          - "1,000 ×§×˜×’×•×¨×™×•×ª ×©×•× ×•×ª"
          - "×”××•×“×œ ××–×”×” ××” ×‘×ª××•× ×”"
      - type: image
        src: "/assets/images/image-net.png"
        alt: "ImageNet Classification Task"

  # -------------------------------------------
  # Convolutions (2 slides)
  # -------------------------------------------
  - id: slide-033
    layout: title
    direction: rtl
    content:
      - type: title
        text: "×§×•× ×‘×•×œ×•×¦×™×•×ª"
      - type: subtitle
        text: "Convolutions - ××™×š ×”×¨×©×ª ×¨×•××” ×ª××•× ×•×ª"

  - id: slide-034
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "×¤×¢×•×œ×ª ×”×§×•× ×‘×•×œ×•×¦×™×”"
      - type: diagram
        src: "/assets/diagrams/convolution_operation_he.svg"
    notes: |
      TODO: Create diagram showing convolution operation
      - Filter/kernel sliding over image
      - Feature detection (edges, shapes)
      - Pooling layers

  # -------------------------------------------
  # ResNet (2015) - Skip Connections
  # -------------------------------------------
  - id: slide-035
    layout: title
    direction: rtl
    content:
      - type: title
        text: "ResNet (2015)"
      - type: subtitle
        text: "×—×™×‘×•×¨×™ ×§×™×¦×•×¨ - Skip Connections"

  - id: slide-036
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "ResNet - ×”×¨×¢×™×•×Ÿ ×©××¤×©×¨ ×¨×©×ª×•×ª ×¢××•×§×•×ª ×××•×“"
      - type: diagram
        src: "/assets/diagrams/resnet_skip_connections_he.svg"
    notes: |
      ResNet ×¤×ª×¨ ××ª ×‘×¢×™×™×ª ×”-Degradation: ×¨×©×ª×•×ª ×¢××•×§×•×ª ×™×•×ª×¨ × ×ª× ×• ×ª×•×¦××•×ª ×’×¨×•×¢×•×ª ×™×•×ª×¨.
      ×”×¤×ª×¨×•×Ÿ: Skip Connections - ×”×§×œ×˜ ××“×œ×’ ×™×©×™×¨×•×ª ×œ×¤×œ×˜.
      ×”××™× ×˜×•××™×¦×™×”: ×× ×”×©×›×‘×•×ª ×œ× ××•×¢×™×œ×•×ª, ×”×¨×©×ª ×™×›×•×œ×” "×œ×“×œ×’" ×¢×œ×™×”×Ÿ.
      ×”×¨×¢×™×•×Ÿ ×”×–×” × ××¦× ×”×™×•× ×‘×›×œ ××•×“×œ ×’×“×•×œ ×›×•×œ×œ Transformers!

  # -------------------------------------------
  # Bag of Words (2 slides)
  # -------------------------------------------
  - id: slide-010
    layout: title
    direction: rtl
    content:
      - type: title
        text: "Bag of Words"
      - type: subtitle
        text: "×™×™×¦×•×’ ×˜×§×¡×˜ ×‘×¡×™×¡×™"

  - id: slide-011
    layout: fullDiagram
    content:
      - type: title
        text: "Bag of Words"
      - type: diagram
        src: "/assets/diagrams/bag-of-words.svg"

  # -------------------------------------------
  # Tokens - How AI Reads Text
  # -------------------------------------------
  - id: slide-015
    layout: title
    direction: rtl
    content:
      - type: title
        text: "×˜×•×§× ×™×"
      - type: subtitle
        text: "××™×š ××•×“×œ×™× ×§×•×¨××™× ×˜×§×¡×˜?"

  - id: slide-016
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××˜×§×¡×˜ ×œ×˜×•×§× ×™×"
      - type: diagram
        src: "/assets/diagrams/tokenization_he.svg"
    notes: |
      ×”××•×“×œ ×œ× ××‘×™×Ÿ ×˜×§×¡×˜ ×™×©×™×¨×•×ª - ×”×•× ×¢×•×‘×“ ×¢× ××¡×¤×¨×™×.
      Tokenizer ××¤×¨×§ ××ª ×”×˜×§×¡×˜ ×œ×—×œ×§×™× ×§×˜× ×™× (×˜×•×§× ×™×) ×•×××™×¨ ×›×œ ××—×“ ×œ××¡×¤×¨.
      ×©×™××• ×œ×‘: "playing" ××ª×¤×¨×§ ×œ-"play" + "##ing" - ×–×” subword tokenization.

  - id: slide-017
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "×œ××” Subword Tokenization?"
      - type: diagram
        src: "/assets/diagrams/subword_tokenization_he.svg"
    notes: |
      ×”×‘×¢×™×”: ×× ×›×œ ××™×œ×” ×”×™× ×˜×•×§×Ÿ × ×¤×¨×“, ×¦×¨×™×š ××™×œ×•×Ÿ ×¢× ×§.
      ×”×¤×ª×¨×•×Ÿ: ×œ×¤×¨×§ ××™×œ×™× ×œ×—×œ×§×™× - ×©×•×¨×©, ×ª×—×™×œ×™×ª, ×¡×™×•××ª.
      ×™×ª×¨×•× ×•×ª: ××™×œ×•×Ÿ ×§×˜×Ÿ (~30K), ×›×œ ××™×œ×” × ×™×ª× ×ª ×œ×™×™×¦×•×’, ××™×œ×™× ×—×“×©×•×ª ×œ× ×‘×¢×™×”.

  # -------------------------------------------
  # Word2Vec (2 slides)
  # -------------------------------------------
  - id: slide-020
    layout: title
    direction: rtl
    content:
      - type: title
        text: "Word2Vec"
      - type: subtitle
        text: "××™×œ×™× ×›×•×§×˜×•×¨×™× (2013)"

  - id: slide-021
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "Word2Vec - ××™×œ×™× ×›×•×§×˜×•×¨×™×"
      - type: diagram
        src: "/assets/diagrams/word2vec_basics_he.svg"

  - id: slide-022
    layout: fullDiagram
    content:
      - type: title
        text: "Word2Vec - ××¨×™×ª××˜×™×§×” ×¡×× ×˜×™×ª"
      - type: diagram
        src: "/assets/diagrams/word2vec_arithmetic.svg"

  - id: slide-023
    layout: iframe
    direction: rtl
    content:
      - type: title
        text: "Word2Vec - ××¨×—×‘ ×ª×œ×ª-×××“×™"
      - type: iframe
        src: "/assets/visualizations/word2vec_3d_galaxy.html"

  # -------------------------------------------
  # RNN (2 slides)
  # -------------------------------------------
  - id: slide-040
    layout: title
    direction: rtl
    content:
      - type: title
        text: "×¨×©×ª×•×ª × ×•×™×¨×•× ×™× ×—×•×–×¨×•×ª"
      - type: subtitle
        text: "Recurrent Neural Networks (RNN)"

  - id: slide-041
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××¨×›×™×˜×§×˜×•×¨×ª RNN"
      - type: diagram
        src: "/assets/diagrams/rnn_architecture_he.svg"
    notes: |
      TODO: Create diagram showing:
      - Sequential processing
      - Hidden state passing between time steps
      - Vanishing gradient problem

  # -------------------------------------------
  # LSTM (2 slides)
  # -------------------------------------------
  - id: slide-050
    layout: title
    direction: rtl
    content:
      - type: title
        text: "LSTM"
      - type: subtitle
        text: "Long Short-Term Memory (1997)"

  - id: slide-051
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××¨×›×™×˜×§×˜×•×¨×ª LSTM"
      - type: diagram
        src: "/assets/diagrams/lstm_architecture_he.svg"
    notes: |
      TODO: Create diagram showing:
      - Cell state (memory)
      - Gates: forget, input, output
      - Solving vanishing gradient problem

  # -------------------------------------------
  # Attention Mechanism (2 slides)
  # -------------------------------------------
  - id: slide-060
    layout: title
    direction: rtl
    content:
      - type: title
        text: "×× ×’× ×•×Ÿ ×”-Attention"
      - type: subtitle
        text: "×”×¨×¢×™×•×Ÿ ×©×”×•×‘×™×œ ×œ×˜×¨× ×¡×¤×•×¨××¨×™× (2014)"

  - id: slide-061
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "Attention ×‘-Seq2Seq"
      - type: diagram
        src: "/assets/diagrams/seq2seq_attention_he.svg"
    notes: |
      ×”×‘×¢×™×”: ×œ×¤× ×™ 2014, ××•×“×œ×™× ×“×—×¡×• ××©×¤×˜ ×©×œ× ×œ×•×§×˜×•×¨ ××—×“ - ×‘××©×¤×˜×™× ××¨×•×›×™× ×”××™×“×¢ ××‘×“.
      ×”×¤×ª×¨×•×Ÿ: ×× ×’× ×•×Ÿ Attention ×××¤×©×¨ ×œ××¤×¢× ×— "×œ×”×¡×ª×›×œ ××—×•×¨×”" ×¢×œ ×›×œ ××™×œ×” ×‘×§×œ×˜.

  # -------------------------------------------
  # Self-Attention & Transformers (2 slides)
  # -------------------------------------------
  - id: slide-070
    layout: text-image
    direction: rtl
    content:
      - type: title
        text: "×”×¨×’×¢ ×©×‘×• ×”×›×œ ×”×©×ª× ×”"
      - type: subtitle
        text: "×™×•× ×™ 2017 - Attention Is All You Need"
      - type: bullets
        items:
          - "\"Attention Is All You Need\" - ×¦×•×•×ª Google Brain"
          - "×”×¦×™×’ ××ª ××¨×›×™×˜×§×˜×•×¨×ª ×”-Transformer"
          - "×”×•×›×™×—: ×œ× ×¦×¨×™×š RNN ××• CNN - ×¨×§ Attention"
          - "×”×‘×¡×™×¡ ×œ: GPT, BERT, ChatGPT, Claude, Gemini..."
      - type: image
        src: "/assets/images/attention-is-all-you-need-paper.svg"
        alt: "Attention Is All You Need - the landmark paper"

  - id: slide-071
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "Self-Attention (×§×©×‘ ×¢×¦××™)"
      - type: subtitle
        text: "×”×¨×¢×™×•×Ÿ ×”××¨×›×–×™ ×‘×˜×¨× ×¡×¤×•×¨××¨×™×"
      - type: diagram
        src: "/assets/diagrams/self_attention_he.svg"
    notes: |
      Self-Attention = ×›×œ ××™×œ×” ×‘××©×¤×˜ "××¡×ª×›×œ×ª" ×¢×œ ×›×œ ×”××™×œ×™× ×”××—×¨×•×ª.
      ×“×•×’××”: "×”×—×ª×•×œ ×™×©×‘ ×¢×œ ×”×©×˜×™×— ×›×™ ×”×•× ×”×™×” ×¢×™×™×£" - "×”×•×" ××ª×™×™×—×¡×ª ×œ"×—×ª×•×œ".

  - id: slide-072
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××˜×¨×™×¦×ª Self-Attention"
      - type: subtitle
        text: "×›×œ ××™×œ×” × ×•×ª× ×ª ×¦×™×•×Ÿ ×œ×›×œ ××™×œ×” ××—×¨×ª"
      - type: diagram
        src: "/assets/diagrams/self_attention_matrix_he.svg"
    notes: |
      ×”××˜×¨×™×¦×” ××¨××” ×›××” ×ª×©×•××ª ×œ×‘ ×›×œ ××™×œ×” × ×•×ª× ×ª ×œ×›×œ ××™×œ×” ××—×¨×ª.
      ×©×•×¨×” = ××™×œ×” ×©×©×•××œ×ª (Query), ×¢××•×“×” = ××™×œ×” ×©×¢×•× ×” (Key).
      ×”×¢×¨×›×™× ××¡×ª×›××™× ×œ-1 ×‘×›×œ ×©×•×¨×” (softmax).

  - id: slide-073
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "××¨×›×™×˜×§×˜×•×¨×ª ×”×˜×¨× ×¡×¤×•×¨××¨"
      - type: subtitle
        text: "×”××‘× ×” ×”××œ× - Encoder-Decoder"
      - type: diagram
        src: "/assets/diagrams/transformer_architecture_he.svg"
    notes: |
      ×”×˜×¨× ×¡×¤×•×¨××¨ ×”××§×•×¨×™ ×›×•×œ×œ ××§×•×“×“ (Encoder) ×•××¤×¢× ×— (Decoder).
      ×›×œ ×‘×œ×•×§ ××›×™×œ: Multi-Head Attention, Feed Forward, Add & Norm.
      Skip Connections ×××¤×©×¨×™× ××™××•×Ÿ ×©×œ ×¨×©×ª×•×ª ×¢××•×§×•×ª ×××•×“.

  # -------------------------------------------
  # Modern Models: BERT, GPT, T5 (2 slides)
  # -------------------------------------------
  - id: slide-080
    layout: title
    direction: rtl
    content:
      - type: title
        text: "××•×“×œ×™× ××•×“×¨× ×™×™×"
      - type: subtitle
        text: "BERT, GPT, T5 ×•×¢×•×“"

  - id: slide-081
    layout: fullDiagram
    direction: rtl
    content:
      - type: title
        text: "×”×©×•×•××ª ××•×“×œ×™× ××•×“×¨× ×™×™×"
      - type: diagram
        src: "/assets/diagrams/modern_models_comparison_he.svg"
    notes: |
      TODO: Create diagram/table comparing:
      - BERT (2018) - Encoder only, bidirectional, good for understanding
      - GPT (2018-2024) - Decoder only, autoregressive, good for generation
      - T5 (2019) - Encoder-Decoder, text-to-text framework
      - Key differences and use cases

  # ===========================================
  # END
  # ===========================================
  - id: slide-099
    layout: blank
    content: []
